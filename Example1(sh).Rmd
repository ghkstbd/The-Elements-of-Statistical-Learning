---
title: "Example1(sh)"
author: "2019712088 이승환"
date: '2020 10 25'
header-includes:
  - \usepackage{kotex}
  - \usepackage{fontspec}
  - \usepackage{unicode-math}
output:
  pdf_document:
    latex_engine: xelatex 
  bookdown::pdf_document2:
    latex_engine: xelatex
  html_document:
    df_print: paged
  word_document: default
mainfont: NanumGothic
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Statistical Data Mining Example 1 =================================
setwd("C:/Users/ghkstbd/Desktop/2020 고급데이터마이닝/R Example 1")
############### Q1. ################# 
# red points form training daaset
redpoints=read.table("trainred.txt",sep="\t",header=FALSE)

# green points from training dataset
greenpoints=read.table("traingreen.txt",sep="\t",header=FALSE)

# red points from test dataset
redtestpoints=read.table("testred.txt",sep="\t",header=FALSE)

# green points from test dataset
greentestpoints=read.table("testgreen.txt",sep="\t",header=FALSE)

# Combine red and green points.
dim(redpoints) # 100 by 2
dim(greenpoints) # 100 by 2
X=rbind(redpoints,greenpoints) # 1~100:red, 101~200:green
dim(X) # 200 by 2

y=c(rep(1,100),rep(0,100))
y # 1~100:red(1), 101~200:green(0)

############### Q2. ################# 

par(mfrow=c(1,3))

plot(redpoints,col="red")
plot(greenpoints,col="green")
plot(X,type="n")

par(mfrow=c(1,1))
plot(X,type="n",main='Red & Green',xlab='x1',ylab='x2') # type="n":nothing
points(redpoints,col="red")
points(greenpoints,col="green")

############### Q3. ################# 

is.matrix(X)

is.data.frame(X)

X=as.matrix(X)

is.matrix(X)

# Regression 1
X1=cbind(rep(1,nrow(X)),X) # design matrix : 200 by 3
dim(X1)
beta.hat=solve(t(X1)%*%X1)%*%t(X1)%*%y
beta.hat

y.hat=X1%*%beta.hat # 200 by 1

result <- ifelse(y.hat>0.5,1,0)
result[,1]
sum(result[,1]==y)
te=1-sum(result[,1]==y)/length(y); te

# Regression 2
fit=lm(y~X[,1]+X[,2])
summary(fit)
summary(fit)$coef
summary(fit)$coef[,1]
beta.hat2 = fit$coefficients
y.hat2 = fit$fitted.values # y.hat2 = fitted.values(fit)

g.hat=as.numeric(y.hat>0.5) # T/F로 반환해서 numeric으로 1/0
g.hat
z=sum(g.hat==y)
training.error=1-z/200 # 오분류율을 찾는 거니까!
training.error # 0.225


############### Q4. ################# 

plot(X,type="n",main='Red & Green',xlab='x1',ylab='x2') # type="n":nothing
points(redpoints,col="red")
points(greenpoints,col="green")
beta.hat
abline((.5-beta.hat[1])/beta.hat[3],-beta.hat[2]/beta.hat[3])
#abline((.5-beta.hat[1,1])/beta.hat[3,1],-beta.hat[2,1]/beta.hat[3,1])

############### Q5. ################# 

X0=rbind(redtestpoints,greentestpoints) # testset은 각 1000 by 2
X0=as.matrix(X0) # X0는 2000 by 2

y0=c(rep(1,1000),rep(0,1000)) # 마찬가지로 y를 1~1000(red)=1, 1001~2000(green)=0
X01=cbind(rep(1,2000),X0) # design matrix(2000 by 3)

# 추정한 모수들을 이용하여 y=xb, 즉 testset에서의 예측된 색(y)을 구함, testerror
y0.hat=X01%*%beta.hat
g0.hat=as.numeric(y0.hat>0.5)

test.error=1-sum(g0.hat==y0)/2000
test.error # 0.245 > 0.225(training error)
# testerror is always larger than training error !!!


############### Q6. ################# 

# Training observations 사이의 거리를 계산해야 함.
# Euclidean distance matrix
D=matrix(0,200,200) # 200 by 200 matrix all values are zero.
dim(D) # -> Nrow of training set is 200
# Training set X is 200 by 2 
for (i in 1:200) for (j in 1:200) D[i,j]=sqrt(sum((X[i,]-X[j,])^2))
# => D 200 by 200 계산으로 쭉 채움

# To find k-closest observation, Example
s=c(1,4,5,3)
sort(s) # 1 3 4 5 (ascending dataset)
order(s) # ordered position of dataset, it return the index value
# 1 4 2 3
# 첫 번째로 작은 값은 첫 번째 인덱스(1)에 있다.
# 두 번째로 작은 값은 네 번째 인덱스(3)에 있다.
# 세 번째로 작은 값은 두 번째 인덱스(4)에 있다.
# 네 번째로 작은 값은 세 번째 인덱스(5)에 있다.

# ex) (x1,x1)~(x1,x200)중 closest training obs 찾기
# D[1,] : (x1,x1) ~ (x1,x200)의 거리를 나타냄
order(D[1,])
order(D[1,])[1:7] # 7-closest training obs 

y # y=c(rep(1,100),rep(0,100))
y[order(D[1,])[1:7]]
mean(y[order(D[1,])[1:7]]) # y_hat value
g.hat=(mean(y[order(D[1,])[1:7]])>0.5)
g.hat



# Q6.

k=7
g.hat7=rep(0,200)
for (i in 1:200) g.hat7[i]=(mean(y[order(D[i,])[1:k]])>0.5)
 g.hat7
 training.error=1-sum(g.hat7==y)/200
 training.error

 
k=3
g.hat3=rep(0,200)
 for (i in 1:200) g.hat3[i]=(mean(y[order(D[i,])[1:k]])>0.5)
 training.error=1-sum(g.hat3==y)/200
 training.error

k=1
g.hat1=rep(0,200)
 for (i in 1:200) g.hat1[i]=(mean(y[order(D[i,])[1:k]])>0.5)
 training.error=1-sum(g.hat1==y)/200
 training.error
 
g <- NULL
for(k in c(1,3,7)){
        for(i in 1:200){
                g[i] <- mean(y[order(D[i,])[1:k]])>0.5
        }
        print(1-sum(g==y)/200)
}
 
############### Q7. ################# 
# Test observations 사이의 거리를 계산해야 함.
# Euclidean distance matrix
D0=matrix(0,2000,200) # row = testset, col = trainingset
dim(D0)

# (x1,x1)~(x1,x200)~...~(x2000,x1)~(x2000,x200)
for (i in 1:2000) for (j in 1:200) D0[i,j]=sqrt(sum((X0[i,]-X[j,])^2))


k=7
g0.hat=rep(0,2000)
for (i in 1:2000) g0.hat[i]=(mean(y[order(D0[i,])[1:k]])>0.5)
head(g0.hat,100)
test.error=1-sum(g0.hat==y0)/2000
test.error

k=3
g0.hat=rep(0,2000)
for (i in 1:2000) g0.hat[i]=(mean(y[order(D0[i,])[1:k]])>0.5)
head(g0.hat,100)
test.error=1-sum(g0.hat==y0)/2000
test.error

k=1
g0.hat=rep(0,2000)
for (i in 1:2000) g0.hat[i]=(mean(y[order(D0[i,])[1:k]])>0.5)
head(g0.hat, 100)
test.error=1-sum(g0.hat==y0)/2000
test.error


g2 <- NULL
for(k in c(1,3,7)){
        for(i in 1:2000){
                g2[i] <- mean(y[order(D0[i,])[1:k]])>0.5
        }
        print(1-sum(g2==y0)/2000)
}
```